#!/usr/bin/env python
# encoding: utf-8

# GNU GPL LICENSE
#
# This module is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; latest version thereof,
# available at: <http://www.gnu.org/licenses/gpl.txt>.
#
# This module is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this module; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA

"""bc-evaluate [--options] <result file(s...)> <standard file>

Official BioCreative evaluation script.
For help on the script options use -h/--help.
Single-char options can be combined (instead of "-c -e", write "-ce").

Evaluation functions
====================

For ACT, the official evaluations use Matthew's Correlation Coefficient (MCC)
to evaluate classification performance and the area under the precision-
recall curve (AUC P/R) for the ranking performance. In addition, specificity,
sensitivity, and accuracy can be measured.

For all other tasks, the official evaluations use the balanced F-measure
(F-Score) to evaluate classification performance and Average Precision (AP)
to measure average ranking performance across all articles. AP is generated
from the precision values calculated for the total TP and FP counts summed up
from all articles at each rank. In addition, precision and recall are
provided, and the macro-averaged results of all these functions with the
corresponding standard deviations can be measured, too (Although, commonly
the std. dev. will be too large and thus the average has no meaningful
interpretation.)

For the ACT the AUC P/R, and for the other tasks the Avrg. Precision can be
plotted if matplotlib is installed. Any evaluation can also be made excluding
the articles not reported by the classifier (results), using the "-e" option.

Input file formats
==================

Result files have to conform to the an official BioCreative result format
used in the challenge.
It is possible send multiple result files to this tool, either by listing one
after the other (multiple arguments) or using the wildcard operators of your
OS (usually, * and ?) in the path to the file(s).

Notes about all input file formats:
Each column has to be separated by tabs, each row by newline characters.
Columns allowed for each evaluation task are listed below; columns starting
with "-" are mandatory, "+" are optional. Note that the format and content
can be ensured with the strict option, i.e., when using that option the input
files may only and exclusively contain the columns described here. If you
have malformed data anywhere it is possible to get unpredictable results,
errors and/or warnings. The strict option should help ensure correct file
formats. No matter wether an optional item is used or not, the order in which
the columns are presented here has to be maintained. Note that not using the
optional columns requires the use of a special command line
parameter (-c for confidence only, -r for rank only, or -l for none). Gold
standard files must NOT have these optional columns, obviously.

ACT - article classification format:
- article identifier (DOI, PMID, etc. - in essence, some unique ID)
- classification (true/false, as either "t"/"f", 0/1, "true"/"false",
  "+"/"-", or "y"/"n", all case insensitive where applicable)
+ rank (a unique integer > 0 for that classification type - i.e., all true
  classifications and all false classifications each have to have unique
  ranks).
+ confidence (a floating point number in the range (0,1], not more precise
  than the decimal precision available to Python on your OS)
 
IMT - detection method format (same as INT):
INT - protein normalization format:
- article identifier (DOI, PMID, etc. - in essence, some unique ID)
- INT: protein identifier (UniProt accession, in essence any string)
  IMT: detection method ID (OBO PSI:MI term ID, in essence any string)
+ rank (same as ACT, but the rank has to be unique wrt. the article ID)
+ confidence (same as ACT)

IPT - protein pairs format:
- article identifier (DOI, PMID, etc. - in essence, some unique ID)
- protein A identifier (UniProt accession, in essence any string)
- protein B identifier (UniProt accession, in essence any string)
+ rank (same as INT)
+ confidence (same as ACT)

Protein pairs special note:
It is of no relevance in which order a pair is given ("A B" or "B A"),
however, the program will ensure that each pair is unique for a given article
and reports an error if not.

Output formats (command line options)
=====================================

See the output of "-h"/"--help" for a list of all command line options and
their description.
In summary, it is possible to generate:
a) a verbose result report (default)
c) a tab-separated list of all scores, including a header line
b) just the single numeric value of a single performance measure
c) a tab-separated list of all precision-recall values
d) a plot of the AUC P/R curve (ACT) or AP step-curve (other tasks); this
   requires matplotlib installed; see: http://matplotlib.sourceforge.net/
e) debugging output - of input files after pre-processing, but before the
   actual calculations; e.g., to print the updated result file after ranks
   have been added (if they were not given), or after homonym ortholog
   mapping or organism filtering.

Comments and observations
=========================

By default, the script assumes: INT/protein identifier data, where the
results have a column for rank and confidence scores, using verbose output.
To add to/change this behavior use one or more of the many the command line
options. In addition, with the "--error", "--info" and "--debug" flags, the
verbosity of the program (logs to STDERR by default) can be influenced.
Especially if getting errors when running the program instead of seeing a
result, it is recommended to use --info to get more logging messages about
what exactly is wrong.

The line ordering option (-l, --line) means that no rank or confidence was
given and instead the program should use the intrinsic line ordering of the
input file as the rank.

For INT/normalizations and IPT/pairs ranking is straightforward.
For measuring ACT AUC P/R, however, the script will traverse the results from
the highest ranking true classification to the lowest true classification and
then from the LOWEST ranking false classification to the HIGHEST (i.e.,
reversing the order). This means, if only line ordering is used, an input
file like this:
A   t
B   f
C   t
D   f
will result in the following order: A, C, D, B. To use only the confidence
scores to order your results, simply use the -c/--confidence flag only
(i.e., without the -l or -r flag). Again, for ACT ranking evaluation, the
same principle of reverse ordering for the negative results is applied.

Note that for ACT evaluation, specificity, sensitivity, accuracy, and
Matthew's Correlation Coefficient are calculated by using the classification
and these values are not influenced by ranks, while AUC P/R only uses the
rank but ignores the actual classification of the result.

Optional evaluations
====================

Both the ortholog mapping and organism filtering can either be carried out
each individually or in concert, i.e., first all possible mappings get made,
then all proteins that still do not belong to the correct organisms _after_
the mapping step are filtered. Note that this double step produces somewhat
over-optimized results: this is the best achievable result if a human being
would

1) first chose the correct organisms and then
2) (!) also chose the correct mappings.

While evaluating filtering exclusively produces results where the simulated
human interaction is limited to choosing the correct organisms only and
therefore might be of more public interest, while mapping and the combined
evaluation can be understood as a "maximized performance optimistic".

Homonym ortholog mapping
------------------------

This evaluation helps detect possible errors in the protein name dictionary.

For any protein identifier in the gold standard (INT, IPT), a list of
homonym orthologs can be given; any identifier in the results that can be
mapped back to a correct gold standard entry with this mapping file will be
counted as TP; If a result can be mapped to several gold standard
annotations, all these mappings are added; If any mapping operation results
in multiple annotations of the same identifier/pair on the article, the
highest ranking annotation is kept and all others are discarded.
File format: Gold standard accession (string) followed by a tabulator,
followed by comma-separated homonym ortholog accessions (strings)

Organism filtering
------------------

This evaluation helps detect problems with establishing the focus organisms.

The map should contain all taxonomic IDs of the used UniProt release for all
accessions and at least must contain the protein identifiers found in the
gold standard. For each article, all relevant taxonomic identifiers are
established from the gold standard and then any protein identifier found in
the results that does not map to one of those taxonomic IDs is removed.
Also, if the protein identifier does not have a mapping according to this
list, it is removed.
Note that due to the large size of UniProt, loading this file can take quite
some time on slow machines.
File format: UniProt accession (string) followed by a tabulator, followed by
a taxonomic identifier (in essence, any string).

========================================================
Created by Florian Leitner on 2009-10-14 at CNIO.
Copyright (c) Florian Leitner. All rights reserved.
License: GNU Public License, latest version."""

import logging
import os
import sys

from optparse import OptionParser

# File I/O imports
from biocreative.evaluation.file_io import \
    gold_standard_reader_factory, result_reader_factory
from biocreative.evaluation.file_io.homonym_ortholog import \
    HomonymOrthologReader
from biocreative.evaluation.file_io.protein_organism import \
    ProteinOrganismReader
from biocreative.evaluation.file_io.store import Files

# all others
from biocreative.evaluation.graphics import plot_avrg_p_curves
from biocreative.evaluation.manager import Manager
from biocreative.evaluation.parameters import Parameters
from biocreative.evaluation.settings import Defaults, Evaluate

__author__ = "Florian Leitner"
__version__ = "3.0.0"

# ===================
# = Output Handling =
# ===================

class Output(object):
    """Output modes available by this script."""
    
    verbose = 'verbose'
    tabular = 'tabular'
    avrg_p = 'avrg_p'
    auc_pr = 'auc_pr'
    f_score = 'f_score'
    recall = 'recall'
    precision = 'precision'
    sensitivity = 'sensitivity'
    specificity = 'specificity'
    accuracy = 'accuracy'
    mcc_score = 'mcc_score'
    pr_values = 'pr_values'
    documents= 'documents'
    
    @staticmethod
    def check_mode(output):
        if output in dir(Output):
            return output
        
        logging.error("unknown output mode '%s'" % str(output))
        return Output.verbose
    

class OutputHandler(object):
    """Formated output to a given file descriptor/handle."""
    
    def __init__(
        self, filepath, evaluation_type,
        output_mode=Output.tabular, plot_result=False
    ):
        if evaluation_type == Evaluate.ACT:
            self._print_data = getattr(self, "_" + output_mode + "_ACT")
        else:
            self._print_data = getattr(self, "_" + output_mode)
        
        self.logger = logging.getLogger('OutputHandler')
        self.file = filepath
        self.output_mode = output_mode
        self.plot_result = plot_result
        self.evaluation_type = evaluation_type
        self.result_name = None
    
    def debug(self, data_dict):
        # debugging data output: the file is always overwritten
        self._fh = self.file.open(mode='w')
        keys = data_dict.keys()
        
        for doi in keys:
            values = data_dict[doi]
            
            if isinstance(values, list):
                for item in values:
                    self.__p("%s\t%s" % (doi, str(item)))
            else:
                self.__p("%s\t%s" % (doi, str(values)))
        
        self.file.close()
    
    def print_data(self, primary, secondary, result_name=None):
        self.logger.debug(
            "calcuating and printing %s data" % self.output_mode
        )
        self.result_name = result_name
        self._fh = self.file.open(mode='a')
        self._print_data(primary, secondary)
        self.file.close()
        
        if self.plot_result:
            plot_avrg_p_curves(primary, self.evaluation_type)
    
    def _verbose_ACT(self, pr_data, mcc_acc_data):
        hits = mcc_acc_data.hits
        self.__p("=======================%s=" % (
            "=" * len(self.result_name)
        ))
        self.__p("Evaluation result for '%s'" % self.result_name)
        print >> self._fh
        self.__p("Classification results")
        print >> self._fh
        self.__p("TP: %3i\tFP: %3i\tFN: %3i\tTN: %3i" % (
            hits.tp, hits.fp, hits.fn, hits.tn
        ))
        self.__p("sensv.:\t%.5f\tspecf.:\t%.5f\taccur.:\t%.5f" % (
            mcc_acc_data.sensitivity, mcc_acc_data.specificity,
            mcc_acc_data.accuracy
        ))
        self.__p("Matthew's correlation coefficient:\t%.5f" % 
            mcc_acc_data.mcc_score
        )
        hits = pr_data.hits
        assert hits.fn == 0, "P/R data has FN counts"
        assert hits.tn == 0, "P/R data has TN counts"
        p_at_full_r = pr_data.p_at_full_r
        print >> self._fh
        self.__p("Ranking results")
        print >> self._fh
        self.__p("P at full R:\t%s" % (
            "%.5f" % p_at_full_r if p_at_full_r is not None else "n/a"
        ))
        print  >> self._fh, "AUC P/R:\t%.5f" % pr_data.auc_pr

    def _verbose(self, micro_data, macro_data):
        hits = micro_data.hits
        self.__p("=======================%s=" % (
            "=" * len(self.result_name)
        ))
        self.__p("Evaluation result for '%s'" % self.result_name)
        self.__p(
            "Evaluated documents:  %9i" % len(macro_data)
        )
        self.__p(
            "Evaluated results:    %9i" % (hits.tp + hits.fp)
        )
        print >> self._fh
        self.__p(
            "Hits\tTP: %3i FP: %3i FN: %3i" % (hits.tp, hits.fp, hits.fn)
        )
        assert hits.tn == 0, "evaluation data has TN counts"
        print >> self._fh
        self.__p("Macro-averaged results")
        print >> self._fh
        self.__p("StdDev\tprecs.:\t%.5f\trecall:\t%.5f" % (
            macro_data.std_dev('precision'),
            macro_data.std_dev('recall')
        ))
        self.__p("StdDev\tF-scr.:\t%.5f\tAvrg P:\t%.5f" % (
            macro_data.std_dev('f_score'),
            macro_data.std_dev('avrg_p')
        ))
        print >> self._fh
        self.__p("Macro\tprecs.:\t%.5f\trecall:\t%.5f" % (
            macro_data.precision, macro_data.recall, 
        ))
        self.__p("Macro\tF-scr.:\t%.5f\tAvrg P:\t%.5f" % (
            macro_data.f_score, macro_data.avrg_p
        ))
        print >> self._fh
        self.__p("Baseline results")
        print >> self._fh
        self.__p("precs.:\t%.5f\trecall:\t%.5f" % (
            micro_data.precision, micro_data.recall
        ))
        self.__p("F-scr.:\t%.5f\tAvrg P:\t%.5f" % (
            micro_data.f_score, micro_data.avrg_p
        ))

    def _yield_string_formated_items(self, values):
        for item in values:
            if isinstance(item, float):
                yield "%.5f" % item
            elif isinstance(item, int):
                yield "%5i" % item
            elif isinstance(item, basestring):
                yield item
            elif item is None:
                yield "n/a"
            else:
                raise RuntimeError, "unknown %s ('%s') to format" % (
                    str(type(item)), str(item)
                )
    
    def _tabular_ACT(self, pr_data, mcc_acc_data):
        hits = mcc_acc_data.hits
        items = [
            self.result_name, hits.tp, hits.fp, hits.fn, hits.tn,
            mcc_acc_data.specificity, mcc_acc_data.sensitivity,
            mcc_acc_data.accuracy, mcc_acc_data.mcc_score,
            pr_data.p_at_full_r, pr_data.auc_pr,
        ]
        self.__p("\t".join(self._yield_string_formated_items(items)))
    
    def _tabular(self, macro_data, micro_data):
        hits = micro_data.hits
        items = [
            self.result_name, len(macro_data), # = num evaluated documents
            hits.tp, hits.fp, hits.fn,
            macro_data.precision, macro_data.std_dev('precision'),
            macro_data.recall, macro_data.std_dev('recall'),
            macro_data.f_score, macro_data.std_dev('f_score'),
            macro_data.avrg_p, macro_data.std_dev('avrg_p'),
            micro_data.precision, micro_data.recall,
            micro_data.f_score, micro_data.avrg_p,
        ]
        self.__p("\t".join(self._yield_string_formated_items(items)))

    #noinspection PyUnusedLocal
    def _avrg_p(self, macro_data, micro_data):
        self.__p(micro_data.avrg_p)
    
    #noinspection PyUnusedLocal
    def _auc_pr_ACT(self, pr_data, mcc_acc_data):
        self.__p(pr_data.auc_pr)

    #noinspection PyUnusedLocal
    def _mcc_score_ACT(self, pr_data, mcc_acc_data):
        self.__p(mcc_acc_data.mcc_score)
    
    #noinspection PyUnusedLocal
    def _f_score(self, macro_data, micro_data):
        self.__p(micro_data.f_score)
    
    #noinspection PyUnusedLocal
    def _recall(self, macro_data, micro_data):
        self.__p(micro_data.recall)
    
    #noinspection PyUnusedLocal
    def _precision(self, micro_data, macro_data):
        self.__p(micro_data.precision)
    
    #noinspection PyUnusedLocal
    def _sensitivity_ACT(self, pr_data, mcc_acc_data):
        self.__p(mcc_acc_data.sensitivity)
    
    #noinspection PyUnusedLocal
    def _specificity_ACT(self, pr_data, mcc_acc_data):
        self.__p(mcc_acc_data.specificity)
    
    #noinspection PyUnusedLocal
    def _accuracy_ACT(self, pr_data, mcc_acc_data):
        self.__p(mcc_acc_data.accuracy)
    
    #noinspection PyUnusedLocal
    def _pr_values_ACT(self, pr_data, mcc_acc_data):
        self._pr_values(pr_data, mcc_acc_data)
    
    #noinspection PyUnusedLocal
    def _pr_values(self, micro_data, macro_data):
        for p_r in micro_data.yield_precision_recall_pairs():
            self.__p("%f\t%f" % p_r)
    
    def __p(self, data):
        print >> self._fh, data

# ========
# = Main =
# ========

def main(args, opts):
    """Main program entry point; args and opts as parsed by OptionParser."""
    logger = logging.getLogger("main")
    
    # SETUP
    opts.RESULT_ORDER = opts.rank + opts.confidence + opts.line
    params = Parameters(opts)
    file_store = Files(
        gold_standard=args[-1],
        results=args[:-1],
        homonym_orthologs=opts.ho,
        protein_organisms=opts.of,
        output=(sys.stdout if opts.output is None else opts.output),
    )
    debug = (opts.debug_results or opts.debug_gs)
    
    GS_Reader = gold_standard_reader_factory(params.evaluation_type)
    Result_Reader = result_reader_factory(params.evaluation_type)
    Result_Reader.strict = opts.strict
    
    gs_iterator = GS_Reader(file_store.gold_standard, params.field_separator)
    manager = Manager(params.evaluation_type)
    output_handle = OutputHandler(
        file_store.output, params.evaluation_type,
        opts.output_mode, params.plot_result
    )
    
    if opts.ho is not None:
        ho_reader = HomonymOrthologReader(
            file_store.homonym_orthologs, params.field_separator
        )
        # read homonym ortholog map:
        manager.do_homonym_ortholog_mapping(dict(ho_reader))
    
    if opts.of is not None:
        po_reader = ProteinOrganismReader(
            file_store.protein_organisms, params.field_separator
        )
        # read protein organism (tax ID) map:
        manager.do_organism_filtering(dict(po_reader))
    
    # read gold standard:
    try:
        manager.load_gold_standard(gs_iterator)
    except Exception:
        logger.critical("evaluation failed while reading gold standard")
        logger.info("using wrong GS for this evaluation type?")
        
        if logger.isEnabledFor(logging.DEBUG):
            logger.exception("Exception Traceback")
        
        return 1
    
    if opts.output_mode == 'tabular' and \
       not opts.debug_results and not opts.debug_gs:
        if params.evaluation_type == Evaluate.ACT:
            print "run\tTP\tFP\tFN\tTN\tspec\tsens\tacc\tmcc\tp_at_r\tpr"
        else:
            print "run\tdocs\tTP\tFP\tFN\t",
            print "prec_m\tprec_sd\trec_m\trec_sd\t",
            print "f1_m\tf1_sd\tap_ma\tap_sd",
            print "\tprec\trec\tf1\tap"
    
    for result_file in file_store.results:
        results = Result_Reader(
            result_file, params.field_separator, params.result_order
        )
        
        try:
            # ===========================================================
            # ======= The actual evaluation is done by this call. =======
            primary, secondary = manager.evaluate(results, params, debug)
            # ===========================================================
        except Exception:
            logger.critical("evaluation failed for %s" % result_file)
            
            if logger.isEnabledFor(logging.DEBUG):
                logger.exception("Exception Traceback")
            
            return 1
        
        if opts.debug_results:
            if params.cutoff:
                for doi in secondary:
                    secondary[doi] = secondary[doi][0:params.cutoff]
            
            output_handle.debug(secondary)
        elif opts.debug_gs:
            output_handle.debug(primary)
        else:
            output_handle.print_data(
                primary, secondary, result_name=result_file.rootname
            )
    
    return 0

# =============
# = CLI Setup =
# =============

if __name__ == "__main__":
    # Command line parsing setup
    usage = "usage: %prog [--options] <result file(s...)> <standard file>"
    parser = OptionParser(
        usage=usage, version=__version__, prog=os.path.basename(sys.argv[0]),
        epilog="use -d or --documentation to read the instructions"
    )
    parser.set_defaults(EVALUATION_TYPE=Evaluate.INT)
    parser.set_defaults(output_mode=Output.verbose)
    parser.set_defaults(logging=logging.WARNING)
    parser.add_option(
        "-d", "--documentation", action="store_true",
        default=False,
        help="show extensive documentation and exit"
    )
    parser.add_option(
        "--ACT", action="store_const",
        const=Evaluate.ACT, dest="EVALUATION_TYPE",
        help="evaluate ACT/articles [default: %default]"
    )
    parser.add_option(
        "--INT", action="store_const",
        const=Evaluate.INT, dest="EVALUATION_TYPE",
        help="evaluate INT/normalizations [default: %default]"
    )
    parser.add_option(
        "--IMT", action="store_const",
        const=Evaluate.IMT, dest="EVALUATION_TYPE",
        help="evaluate IMT/method detection [default: %default]"
    )
    parser.add_option(
        "--IPT", action="store_const",
        const=Evaluate.IPT, dest="EVALUATION_TYPE",
        help="evaluate IPT/pairs [default: %default]"
    )
    parser.add_option(
        "-c", "--confidence", action="store_const", const=1, default=0,
        help="confidence given; used for ordering if neither -r or -l is used"
    )
    parser.add_option(
        "-r", "--rank", action="store_const", const=10, default=0,
        help="rank given; used for ordering preferentially to confidence"
    )
    parser.add_option(
        "-l", "--line", action="store_const", const=100, default=0,
        help="order by line; overrides ordering by confidence or rank"
    )
    parser.add_option(
        "-s", "--strict", action="store_true", default=False,
        help="ensure columns in result file (no additional columns)"
    )
    parser.add_option(
        "-k", "--cutoff", action="store", type="int",
        default=Defaults.CUTOFF_AT_RANK, dest="CUTOFF_AT_RANK",
        help="only use CUTOFF results per document [default: all]"
    )
    parser.add_option(
        "-e", "--exclude-missed-docs", action="store_true",
        default=Defaults.SKIP_EMPTY_RESULTS, dest="SKIP_EMPTY_RESULTS",
        help="do not evaluate documents without hits [default: %s]" % (
            Defaults.SKIP_EMPTY_RESULTS
        )
    )
    parser.add_option(
        "-v", "--verbose", action="store_const",
        const=Output.verbose, dest="output_mode",
        help="print verbose output [default]"
    )
    parser.add_option(
        "-t", "--tabular", action="store_const",
        const=Output.tabular, dest="output_mode",
        help="print tabular output"
    )
    parser.add_option(
        "--avrg-p", action="store_const",
        const=Output.avrg_p, dest="output_mode",
        help="print Average Precision only"
    )
    parser.add_option(
        "--f-score", action="store_const",
        const=Output.f_score, dest="output",
        help="print F-score only"
    )
    parser.add_option(
        "--recall", action="store_const",
        const=Output.recall, dest="output_mode", 
        help="print recall only"
    )
    parser.add_option(
        "--precision", action="store_const",
        const=Output.precision, dest="output_mode",
        help="print precision only"
    )
    parser.add_option(
        "--mcc-score", action="store_const",
        const=Output.mcc_score, dest="output_mode", 
        help="print (ACT) Matthew's Corr. Coeff. only"
    )
    parser.add_option(
        "--auc-pr", action="store_const",
        const=Output.auc_pr, dest="output_mode",
        help="print (ACT) AUC Precision/Recall only"
    )
    parser.add_option(
        "--accuracy", action="store_const",
        const=Output.accuracy, dest="output_mode", 
        help="print (ACT) accuracy only"
    )
    parser.add_option(
        "--sensitivity", action="store_const",
        const=Output.sensitivity, dest="output_mode", 
        help="print (ACT) sensitivity only"
    )
    parser.add_option(
        "--specificity", action="store_const",
        const=Output.specificity, dest="output_mode", 
        help="print (ACT) specificity only"
    )
    parser.add_option(
        "--pr-values", action="store_const",
        const=Output.pr_values, dest="output_mode",
        help="print P/R value pairs"
    )
    parser.add_option(
        "--output", action="store", type="string",
        help="appends (!) the output to file [default: STDOUT]"
    )
    parser.add_option(
        "--ho", action="store", type="string",
        help="project to homonym orthologs using mapping file"
    )
    parser.add_option(
        "--of", action="store", type="string",
        help="filter by organisms using mapping file"
    )
    parser.add_option(
        "--plot", action="store_true", default=Defaults.PLOT_RESULT,
        dest="PLOT_RESULT",
        help="plot the AP or P/R curve [default: " \
            "%default]\t(requires matplotlib installed)"
    )
    parser.add_option(
        "--error",
        action="store_const", const=logging.ERROR, dest="logging",
        help="error output only [default: warn]"
    )
    parser.add_option(
        "--info",
        action="store_const", const=logging.INFO, dest="logging",
        help="add info output [default: warn]"
    )
    parser.add_option(
        "--debug",
        action="store_const", const=logging.DEBUG, dest="logging",
        help="add info and debug output [default: warn]"
    )
    parser.add_option(
        "--debug-results", action="store_true", default=False,
        help="prints the results data after reading, ordering, and filtering"
    )
    parser.add_option(
        "--debug-gs", action="store_true", default=False,
        help="prints the GS data after reading it; " \
             "both debug-X options overwrite (!) the --output file if given"
    )
    opts, args = parser.parse_args()
    logging.basicConfig(
        level=opts.logging,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
        datefmt="%H:%M:%S"
    )
    
    if opts.documentation:
        print __doc__
        sys.exit(1)
    
    if not len(args) > 1:
        parser.error("insufficient arguments (%i)" % len(args))
    else:
        try:
            tmp = [open(fn, 'r') for fn in args]
        except IOError, io_ex:
            if opts.logging == logging.DEBUG:
                logging.exception("could not open input file")
            
            parser.error(str(io_ex))
    
    if opts.EVALUATION_TYPE == Evaluate.ACT:
        opts.SKIP_EMPTY_RESULTS = False
        
        if opts.CUTOFF_AT_RANK:
            parser.error("cutoff n/a to ACT evaluation")
        elif opts.output_mode in (
            Output.recall, Output.precision, Output.f_score, Output.avrg_p
        ):
            parser.error(
                "%s n/a to ACT evaluation" % opts.output_mode
            )
    else:
        if opts.output_mode in (
            Output.sensitivity, Output.specificity, Output.accuracy,
            Output.mcc_score, Output.auc_pr
        ):
            parser.error(
                "%s n/a to INT, IMT, or IPT evaluation" % opts.output_mode
            )
    
    sys.exit(main(args, opts))

