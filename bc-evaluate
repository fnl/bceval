#!/usr/bin/env python
# encoding: utf-8

# GNU GPL LICENSE
#
# This module is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; latest version thereof,
# available at: <http://www.gnu.org/licenses/gpl.txt>.
#
# This module is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this module; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA

"""bc-evaluate [--options] <result file(s...)> <standard file>

Official BioCreative evaluation script.
For help on the script options use -h/--help.

Input file formats
==================

Result files have to an official BioCreative result format used in the
challenge.
You can send multiple result files to the tool, either by listing one after
the other (multiple arguments) or using the wildcard operators of your OS
(usually, * and ?) in the path to the file(s).

Notes about all input file formats:
Each column has to be separated by tabs, each row by newline characters.
Columns allowed for each evaluation task are listed below; columns starting
with "-" are mandatory, "+" are optional. Note that the format and content
is strict, i.e. your input files may only and exclusively contain the columns
described here. If you have additional data anywhere in these files, you will
get unpredictable errors and/or warnings. No matter wether an optional item
is used or not, the order in which the columns are presented has to be
maintained (note that not using the optional columns usually requires the
use of a special command line parameter). Gold standard files will NOT have
the optional columns, obviously.

ACT - article classification format:
- article identifier (DOI, PMID, etc. - in essence, some unique ID)
- classification (true/false, as either "t"/"f", 0/1, "true"/"false",
  "+"/"-", or "y"/"n", all case insensitive where applicable)
+ rank (a unique integer > 0 for that classification type - i.e., all true
  classifications and all false classifications each have to have unique
  ranks).
+ confidence (a floating point number in the range (0,1], not more precise
  than the decimal precision available to Python on your OS)
 
IMT - detection method format (same as INT):
INT - protein normalization format:
- article identifier (DOI, PMID, etc. - in essence, some unique ID)
- INT: protein identifier (UniProt accession, in essence any string)
  IMT: detection method ID (OBO PSI:MI term ID, in essence any string)
+ rank (same as ACT, but the rank has to be unique wrt. the article ID)
+ confidence (same as ACT)

IPT - protein pairs format:
- article identifier (DOI, PMID, etc. - in essence, some unique ID)
- protein A identifier (UniProt accession, in essence any string)
- protein B identifier (UniProt accession, in essence any string)
+ rank (same as INT)
+ confidence (same as ACT)

Protein pairs special note:
It is of no relevance in which order a pair is given ("A B" or "B A"),
however, the program will ensure that each pair is unique for a given article.

Optional file formats
=====================

Homonym ortholog map:
For any protein identifier in the gold standard (INT, IPT), a list of
homonym orthologs can be given; any identifier in the results that can be
mapped back to the correct gold standard entry with this mapping file will be
counted as TP; If a result can be mapped to several gold standard
annotations, all these mappings are added; If any mapping operations resulted
in multiple annotations of the same identifier/pair on the article, the
highest ranking annotation is kept and all others are discarded.
File format: Gold standard accession (string) followed by a tabulator,
followed by comma-separated homonym ortholog accessions (strings)

[Protein] Organism filtering map:
This map should contain all taxonomic IDs of the used UniProt release for all
accessions and at least must contain the protein identifiers found in the
gold standard. For each article, all relevant taxonomic identifiers are
established and then any protein identifier found in the results that does
not map to one of those taxonomic IDs is removed. Also, if the protein
identifier does not have a mapping according to this list, it is removed.
Note that due to the large size of UniProt, loading this file can take quite
some time on slow machines.
File format: UniProt accession (string) followed by a tabulator, followed by
a taxonomic identifier (in essence, any string).

Both the ortholog mapping and organism filtering can either be carried out
each individually or in concert, i.e., first all possible mappings get made,
then all proteins that still do not belong to the correct organisms _after_
the mapping step are filtered. Note that this double step produces somewhat
optimized results: this is the best achievable result if a human being would
1) first chose the correct organisms and then
2) also chose the correct mappings.
On the contrary, evaluating just with either mapping or filtering exclusively
produces results where the simulated human interaction is limited to choosing
the correct organisms and might be of more technical interest, while the
combination is to be understood as the "maximally achievable performance".

Output formats (command line options)
=====================================

See the output of "-h"/"--help" for a list of all command line options and
their description.
In summary, it is possible to generate:
a) a verbose result report (default)
b) just the single numeric value of a single performance measure
c) a tab-separated list of precision/recall values, including a third column
   which is either empty or starred (i.e., has the asterisk character in it)
   if the column is one of the interpolation points (for AUC iP/R).
d) a plot of the AUC iP/R curve; requires matplotlib installed; see:
   http://matplotlib.sourceforge.net/
e) "debug" output - input files after pre-processing, but before the actual
   calculation; e.g., to print the updated result file after ranks have been
   added (if they were not added), or after homonym ortholog mapping or
   organism filtering.
g) tabulated output with the input file as the first column;
   For ACT, the other columns report:
   col 2-5: TP, FP, FN, TN (relative to classification)
   col 6-9: specificity, sensitivity, accuracy, Matthew's Correlation Coeff.
   col 10-11: precision at full recall (if applicable), AUC iP/R
   For INT and IPT, the other columns report:
   col 2: number of evaluated documents
   col 3-5: global hits: TP, FP, FN
   col 6-9: mIcro-avrgd. precision, recall, F-score, AUC iP/R
   col 10-13: mAcro-avrgd. precision, recall, F-score, AUC iP/R

Comments and observations
=========================

By default, the script assumes: INT/protein identifier data, where the
results have a column for rank and confidence scores, using verbose output.
To add to/change this behavior use one or more of the many the command line
options. In addition, with the "-error", "--info" and "--debug" flags, the
verbosity of the program (logs to STDERR by default) can be influenced.
Especially if you are getting errors when running the program instead of
seeing a result, it is recommended to use --info to get more logging messages
about what is going on.

The line ordering option (-l, --line) means that no rank was given and
instead the program should use the intrinsic line ordering of the input file
as the rank. For INT/normalizations and IPT/pairs this is straightforward.
For ACT however remember that the script will first order the highest ranking
true classification to the lowest true classification and then from the
LOWEST false classification to the HIGHEST (i.e., reversing the order). This
means, if you use line ordering, an input like this:
A   t
B   f
C   t
D   f
will result in the following order: A, C, D, B. To use only the confidence
scores to order your results, simply use the -c/--confidence flag only
(i.e., without the -l or -r flag).

Note that for ACT evaluation, specificity, sensitivity, accuracy, and
Matthew's Correlation Coefficient are calculated by using the classification
and these values are not influenced by the rank, while AUC iP/R only uses the
rank but ignores the actual classification of the result.
For easier use, single-char options can be combined; E.g., to evaluate an
ACT result with confidence ordering, outputting the AUC iP/R score, and
plotting the curve (remember, requires a working installation of matplotlib),
use:
bc-evaluate -cia --plot my_act_result_file act_gold_standard_file
If you would have had confidence values, but wanted to use line ordering on
top of those values, you would change "-cia" to "-clia"; see the -h/--help
output to understand why.

========================================================
Created by Florian Leitner on 2009-10-14 at CNIO.
Copyright (c) 2009-2010 Florian Leitner. All rights reserved.
License: GNU Public License, latest version."""

import logging
import os
import sys

from optparse import OptionParser

# File I/O imports
from biocreative.evaluation.file_io import \
    gold_standard_reader_factory, result_reader_factory
from biocreative.evaluation.file_io.homonym_ortholog import \
    HomonymOrthologReader
from biocreative.evaluation.file_io.protein_organism import \
    ProteinOrganismReader
from biocreative.evaluation.file_io.store import Files

# all others
from biocreative.evaluation.graphics import plot_ipr_curves
from biocreative.evaluation.manager import Manager
from biocreative.evaluation.parameters import Parameters
from biocreative.evaluation.settings import Defaults, Evaluate

__author__ = "Florian Leitner"
__version__ = "2.3.1"

# ===================
# = Output Handling =
# ===================

class Output(object):
    "Output modes available by this script."
    
    verbose = 'verbose'
    tabular = 'tabular'
    auc_ipr = 'auc_ipr'
    f_score = 'f_score'
    recall = 'recall'
    precision = 'precision'
    sensitivity = 'sensitivity'
    specificity = 'specificity'
    accuracy = 'accuracy'
    mcc_score = 'mcc_score'
    pr_values = 'pr_values'
    documents= 'documents'
    
    @staticmethod
    def check_mode(output):
        if output in dir(Output):
            return output
        
        logging.error("unknown output mode '%s'" % str(output))
        return Output.verbose
    

class OutputHandler(object):
    "Formated output to a given file descriptor/handle."
    
    def __init__(
        self, filepath, evaluation_type,
        output_mode=Output.tabular, plot_result=False
    ):
        if evaluation_type == Evaluate.ACT:
            self._print_data = getattr(self, "_" + output_mode + "_ACT")
            self._select = lambda main, secondary: main
        else:
            self._print_data = getattr(self, "_" + output_mode)
            self._select = lambda main, secondary: secondary
        
        self.logger = logging.getLogger('OutputHandler')
        self.file = filepath
        self.output_mode = output_mode
        self.plot_result = plot_result
        self.evaluation_type = evaluation_type
        self.result_name = None
    
    def debug(self, data_dict):
        # debugging data output: the file is always overwritten
        self._fh = self.file.open(mode='w')
        keys = data_dict.keys()
        
        for doi in keys:
            values = data_dict[doi]
            
            if isinstance(values, list):
                for item in values:
                    self.__p("%s\t%s" % (doi, str(item)))
            else:
                self.__p("%s\t%s" % (doi, str(values)))
        
        self.file.close()
    
    def print_data(self, main, secondary, result_name=None):
        self.logger.debug(
            "calcuating and printing %s data" % self.output_mode
        )
        self.result_name = result_name
        self._fh = self.file.open(mode='a')
        self._print_data(main, secondary)
        self.file.close()
        
        if self.plot_result:
            evaluation = self._select(main, secondary)
            plot_ipr_curves(evaluation, self.evaluation_type)
    
    def _verbose_ACT(self, ipr_data, mcc_acc_data):
        hits = mcc_acc_data.hits
        self.__p("=======================%s=" % (
            "=" * len(self.result_name)
        ))
        self.__p("Evaluation result for '%s'" % self.result_name)
        print >> self._fh
        self.__p("TP: %3i\tFP: %3i\tFN: %3i\tTN: %3i" % (
            hits.tp, hits.fp, hits.fn, hits.tn
        ))
        self.__p("sensv.:\t%.5f\tspecf.:\t%.5f\taccur.:\t%.5f" % (
            mcc_acc_data.sensitivity, mcc_acc_data.specificity,
            mcc_acc_data.accuracy
        ))
        self.__p("Matthew's correlation coefficient:\t%.5f" % 
            mcc_acc_data.mcc_score
        )
        hits = ipr_data.hits
        assert hits.fn == 0, "iP/R data has FN counts"
        assert hits.tn == 0, "iP/R data has TN counts"
        p_at_full_r = ipr_data.p_at_full_r
        print >> self._fh
        self.__p("TP: %3i\tFP: %3i" % (hits.tp, hits.fp))
        self.__p("P at full R:\t%s" % (
            "%.5f" % p_at_full_r if p_at_full_r is not None else "n/a"
        ))
        print  >> self._fh,"AUC iP/R:\t%.5f" % ipr_data.auc_ipr
    
    def _verbose(self, macro_data, micro_data):
        hits = micro_data.hits
        hits2 = macro_data.hits
        self.__p("=======================%s=" % (
            "=" * len(self.result_name)
        ))
        self.__p("Evaluation result for '%s'" % self.result_name)
        self.__p(
            "Evaluated documents:  %9i" % len(macro_data)
        )
        self.__p(
            "Evaluated results:    %9i" % (hits.tp + hits.fp)
        )
        print >> self._fh
        assert hits.tn == 0, "evaluation data has TN counts"
        self.__p(
            "Hits\tTP: %3i FP: %3i FN: %3i" % (hits.tp, hits.fp, hits.fn)
        )
        print >> self._fh
        self.__p("Global test-set results (micro-averaged)")
        self.__verbose_helper("Micro", micro_data)
        print >> self._fh
        self.__p("Average per-document results (macro-averaged)")
        self.__p("StdDev\tprecs.:\t%.5f\trecall:\t%.5f\tf-scr.:\t%.5f" % (
            macro_data.std_dev('precision'),
            macro_data.std_dev('recall'),
            macro_data.std_dev('f_score')
        ))
        self.__p("StdDev\tAUC iP/R:\t%.5f" % macro_data.std_dev('auc_ipr'))
        self.__verbose_helper("Macro", macro_data)
    
    def __verbose_helper(self, kind, data):
        self.__p("%s\tprecs.:\t%.5f\trecall:\t%.5f\tf-scr.:\t%.5f" % (
            kind, data.precision, data.recall, data.f_score
        ))
        self.__p("%s\tAUC iP/R:\t%.5f" % (kind, data.auc_ipr))
        
    
    def _yield_string_formated_items(self, values):
        for item in values:
            if isinstance(item, float):
                yield "%.5f" % item
            elif isinstance(item, int):
                yield "%5i" % item
            elif isinstance(item, basestring):
                yield item
            elif item == None:
                yield "n/a"
            else:
                raise RuntimeError, "unknonw %s ('%s') to format" % (
                    str(type(item)), str(item)
                )
    
    def _tabular_ACT(self, ipr_data, mcc_acc_data):
        hits = mcc_acc_data.hits
        items = [
            self.result_name, hits.tp, hits.fp, hits.fn, hits.tn,
            mcc_acc_data.specificity, mcc_acc_data.sensitivity,
            mcc_acc_data.accuracy, mcc_acc_data.mcc_score,
            ipr_data.p_at_full_r, ipr_data.auc_ipr
        ]
        self.__p("\t".join(self._yield_string_formated_items(items)))
    
    def _tabular(self, macro_data, micro_data):
        hits = micro_data.hits
        items = [
            self.result_name, len(macro_data), # = num evaluated documents
            hits.tp, hits.fp, hits.fn,
            micro_data.precision, micro_data.recall, micro_data.f_score,
            micro_data.auc_ipr,
            macro_data.precision, macro_data.recall, macro_data.f_score,
            macro_data.auc_ipr,
        ]
        self.__p("\t".join(self._yield_string_formated_items(items)))
    
    def _auc_ipr_ACT(self, ipr_data, mcc_acc_data):
        self.__p(ipr_data.auc_ipr)
    
    def _auc_ipr(self, macro_data, micro_data):
        self.__p(macro_data.auc_ipr)
    
    def _mcc_score_ACT(self, ipr_data, mcc_acc_data):
        self.__p(mcc_acc_data.mcc_score)
    
    def _f_score(self, macro_data, micro_data):
        self.__p(macro_data.f_score)
    
    def _recall(self, macro_data, micro_data):
        self.__p(macro_data.recall)
    
    def _precision(self, micro_data, macro_data):
        self.__p(macro_data.precision)
    
    def _sensitivity_ACT(self, ipr_data, mcc_acc_data):
        self.__p(mcc_acc_data.sensitivity)
    
    def _specificity_ACT(self, ipr_data, mcc_acc_data):
        self.__p(mcc_acc_data.specificity)
    
    def _accuracyACT(self, ipr_data, mcc_acc_data):
        self.__p(mcc_acc_data.accuracy)
    
    def _pr_values_ACT(self, ipr_data, mcc_acc_data):
        self._pr_values(None, ipr_data)
    
    def _pr_values(self, macro_data, micro_data):
        ipr_values = micro_data.get_interpolated_pr_list()
        
        for p_r in micro_data.yield_precision_recall_pairs():
            if p_r in ipr_values:
                self.__p("%f\t%f\t*" % p_r)
            else:
                self.__p("%f\t%f\t" % p_r)
    
    def __p(self, data):
        print >> self._fh, data

# ========
# = Main =
# ========

def main(args, opts):
    "Main program entry point; args and opts as parsed by OptionParser."
    logger = logging.getLogger("main")
    
    # SETUP
    opts.RESULT_ORDER = opts.rank + opts.confidence + opts.line
    params = Parameters(opts)
    file_store = Files(
        gold_standard=args[-1],
        results=args[:-1],
        homonym_orthologs=opts.ho,
        protein_organisms=opts.of,
        output=(sys.stdout if opts.output is None else opts.output),
    )
    debug = (opts.debug_results or opts.debug_gs)
    
    GS_Reader = gold_standard_reader_factory(params.evaluation_type)
    Result_Reader = result_reader_factory(params.evaluation_type)
    Result_Reader.strict = opts.strict
    
    gs_iterator = GS_Reader(file_store.gold_standard, params.field_separator)
    manager = Manager(params.evaluation_type)
    output_handle = OutputHandler(
        file_store.output, params.evaluation_type,
        opts.output_mode, params.plot_result
    )
    
    if opts.ho is not None:
        ho_reader = HomonymOrthologReader(
            file_store.homonym_orthologs, params.field_separator
        )
        # read homonym ortholog map:
        manager.do_homonym_ortholog_mapping(dict(ho_reader))
    
    if opts.of is not None:
        po_reader = ProteinOrganismReader(
            file_store.protein_organisms, params.field_separator
        )
        # read protein organism (tax ID) map:
        manager.do_organism_filtering(dict(po_reader))
    
    # read gold standard:
    try:
        manager.load_gold_standard(gs_iterator)
    except Exception, e:
        logger.critical("evaluation failed while reading gold standard")
        logger.info("using wrong GS for this evaluation type?")
        
        if logger.isEnabledFor(logging.DEBUG):
            logger.exception("Exception Traceback")
        
        return 1
    
    for result_file in file_store.results:
        results = Result_Reader(
            result_file, params.field_separator, params.result_order
        )
        
        try:
            # ===========================================================
            # ======= The actual evaluation is done by this call. =======
            primary, secondary = manager.evaluate(results, params, debug)
            # ===========================================================
        except Exception, e:
            logger.critical("evaluation failed for %s" % result_file)
            
            if logger.isEnabledFor(logging.DEBUG):
                logger.exception("Exception Traceback")
            
            return 1
        
        if opts.debug_results:
            if opts.CUTOFF_AT_RANK:
                for doi in secondary:
                    secondary[doi] = secondary[doi][0:opts.CUTOFF_AT_RANK]
            
            output_handle.debug(secondary)
        elif opts.debug_gs:
            output_handle.debug(primary)
        else:
            output_handle.print_data(
                primary, secondary, result_name=result_file.rootname
            )
    
    return 0

# =============
# = CLI Setup =
# =============

if __name__ == "__main__":
    "Command line parsing setup."
    
    usage = "usage: %prog [--options] <result file(s...)> <standard file>"
    parser = OptionParser(
        usage=usage, version=__version__, prog=os.path.basename(sys.argv[0]),
        description=None, epilog=None, add_help_option=True
    )
    parser.set_defaults(EVALUATION_TYPE=Evaluate.INT)
    parser.set_defaults(output_mode=Output.verbose)
    parser.set_defaults(logging=logging.WARNING)
    parser.add_option(
        "-d", "--documentation", action="store_true",
        default=False,
        help="show extensive documentation and exit"
    )
    parser.add_option(
        "-a", "--ACT", action="store_const",
        const=Evaluate.ACT, dest="EVALUATION_TYPE",
        help="evaluate ACT/articles [default: %default]"
    )
    parser.add_option(
        "-n", "--INT", action="store_const",
        const=Evaluate.INT, dest="EVALUATION_TYPE",
        help="evaluate INT/normalizations [default: %default]"
    )
    parser.add_option(
        "-m", "--IMT", action="store_const",
        const=Evaluate.IMT, dest="EVALUATION_TYPE",
        help="evaluate IMT/method detection [default: %default]"
    )
    parser.add_option(
        "-p", "--IPT", action="store_const",
        const=Evaluate.IPT, dest="EVALUATION_TYPE",
        help="evaluate IPT/pairs [default: %default]"
    )
    parser.add_option(
        "-c", "--confidence", action="store_const", const=1, default=0,
        help="confidence given; used for ordering if neither -r or -l is used"
    )
    parser.add_option(
        "-r", "--rank", action="store_const", const=10, default=0,
        help="rank given; used for ordering preferentially to confidence"
    )
    parser.add_option(
        "-l", "--line", action="store_const", const=100, default=0,
        help="order by line; overrides ordering by confidence or rank"
    )
    parser.add_option(
        "-s", "--strict", action="store_true", default=False,
        help="ensure columns in result file (no additional columns)"
    )
    parser.add_option(
        "--all", action="store_false",
        default=Defaults.SKIP_EMPTY_RESULTS, dest="SKIP_EMPTY_RESULTS",
        help="also evaluate documents without hits [default: %s]" % (
            not Defaults.SKIP_EMPTY_RESULTS
        )
    )
    parser.add_option(
        "-k", "--cutoff", action="store", type="int",
        default=Defaults.CUTOFF_AT_RANK, dest="CUTOFF_AT_RANK",
        help="only use CUTOFF results per document [default: all]"
    )
    parser.add_option(
        "-v", "--verbose", action="store_const",
        const=Output.verbose, dest="output_mode",
        help="print verbose output [default]"
    )
    parser.add_option(
        "-t", "--tabular", action="store_const",
        const=Output.tabular, dest="output_mode",
        help="print tabular output"
    )
    parser.add_option(
        "-i", "--auc-ipr", action="store_const",
        const=Output.auc_ipr, dest="output_mode",
        help="print (macro-avrgd.) AUC iP/R only"
    )
    parser.add_option(
        "-f", "--f-score", action="store_const",
        const=Output.f_score, dest="output",
        help="print (macro-avrgd.) F-score only"
    )
    parser.add_option(
        "--recall", action="store_const",
        const=Output.recall, dest="output_mode", 
        help="print (macro-avrgd.) recall only"
    )
    parser.add_option(
        "--precision", action="store_const",
        const=Output.precision, dest="output_mode",
        help="print (macro-avrgd.) precision only"
    )
    parser.add_option(
        "--sensitivity", action="store_const",
        const=Output.sensitivity, dest="output_mode", 
        help="print (ACT) sensitivity only"
    )
    parser.add_option(
        "--specificity", action="store_const",
        const=Output.specificity, dest="output_mode", 
        help="print (ACT) specificity only"
    )
    parser.add_option(
        "--accuracy", action="store_const",
        const=Output.accuracy, dest="output_mode", 
        help="print (ACT) accuracy only"
    )
    parser.add_option(
        "--mcc-score", action="store_const",
        const=Output.mcc_score, dest="output_mode", 
        help="print (ACT) Matthew's Correlation Coefficient only"
    )
    parser.add_option(
        "--pr-values", action="store_const",
        const=Output.pr_values, dest="output_mode",
        help="print [micro (for INT/IPR)] P/R value pairs " \
            "and star [*] interpolation points"
    )
    parser.add_option(
        "--output", action="store", type="string",
        help="appends (!) the output to file [default: STDOUT]"
    )
    parser.add_option(
        "--ho", action="store", type="string",
        help="project to homonym orthologs using mapping file"
    )
    parser.add_option(
        "--of", action="store", type="string",
        help="filter by organisms using mapping file"
    )
    parser.add_option(
        "--plot", action="store_true", default=Defaults.PLOT_RESULT,
        dest="PLOT_RESULT",
        help="plot the (i)P/R curve (micro for INT/IPT) [default: " \
            "%default] (requires matplotlib installed)"
    )
    parser.add_option(
        "--error",
        action="store_const", const=logging.ERROR, dest="logging",
        help="error output only [default: warn]"
    )
    parser.add_option(
        "--info",
        action="store_const", const=logging.INFO, dest="logging",
        help="add info output [default: warn]"
    )
    parser.add_option(
        "--debug",
        action="store_const", const=logging.DEBUG, dest="logging",
        help="add info and debug output [default: warn]"
    )
    parser.add_option(
        "--debug-results", action="store_true", default=False,
        help="prints the results data after reading, ordering, and filtering"
    )
    parser.add_option(
        "--debug-gs", action="store_true", default=False,
        help="prints the GS data after reading it; " \
             "both debug-X options overwrite (!) the --output file if given"
    )
    opts, args = parser.parse_args()
    logging.basicConfig(
        level=opts.logging,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
        datefmt="%H:%M:%S"
    )
    
    if opts.documentation:
        print __doc__
        sys.exit(1)
    
    if not len(args) > 1:
        parser.error("insufficient arguments (%i)" % len(args))
    else:
        try:
            tmp = [open(fn, 'r') for fn in args]
        except IOError, io_ex:
            if opts.logging == logging.DEBUG:
                logging.exception("could not open input file")
            
            parser.error(str(io_ex))
    
    if opts.EVALUATION_TYPE == Evaluate.ACT:
        opts.SKIP_EMPTY_RESULTS = False
        
        if opts.CUTOFF_AT_RANK != 0:
            parser.error("option --cutoff n/a for ACT/article evaluation")
        elif opts.output in (
            Output.recall, Output.precision, Output.f_score
        ):
            parser.error(
                "--%s n/a for ACT/interaction evaluation" % opts.output
            )
    else:
        if opts.output in (
            Output.sensitivity, Output.specificity, Output.accuracy,
            Output.mcc_score
        ):
            parser.error(
                "--%s n/a to INT, IMT, and IPT evaluation" % opts.output
            )
    
    sys.exit(main(args, opts))

